{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6220e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80a8248",
   "metadata": {},
   "source": [
    "# pip, import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install transformers datasets\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from collections import defaultdict, Counter, deque\n",
    "import re\n",
    "from itertools import chain\n",
    "from importlib import import_module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import (Dataset,\n",
    "                              DataLoader, \n",
    "                              RandomSampler, \n",
    "                              SequentialSampler, \n",
    "                              TensorDataset)\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import (AutoConfig, \n",
    "                          AutoTokenizer, \n",
    "                          RobertaForSequenceClassification,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorWithPadding,\n",
    "                          EarlyStoppingCallback)\n",
    "from transformers import AdamW\n",
    "from transformers import (get_scheduler, \n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, _LRScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric, load_dataset, Dataset, concatenate_datasets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             precision_recall_curve,\n",
    "                             f1_score,\n",
    "                             auc)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import math\n",
    "import easydict\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb26fbc",
   "metadata": {},
   "source": [
    "# 시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c11c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42, contain_cuda: bool = False):\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  print(f\"Seed set as {seed}\")\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f805f48",
   "metadata": {},
   "source": [
    "# 경로 설정 및 디바이스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74946140",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/content/drive/MyDrive'\n",
    "project_folder = \"DACON\"\n",
    "os.chdir(os.path.join(root_dir,project_folder))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218f6a48",
   "metadata": {},
   "source": [
    "# wandb에 잘못 분류하는 코드 Pair 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a05287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_batch_for_wandb(tokenizer,\n",
    "                          wrong_sample_index,\n",
    "                          input_ids,\n",
    "                          valid_labels,\n",
    "                          valid_predict,\n",
    "                          valid_output,\n",
    "                          ):\n",
    "  num_to_label_dict = {0:'diff', 1:'same',}\n",
    "\n",
    "  wrong_sample_index = np.where(valid_labels!=valid_predict)[0]\n",
    "  wrong_sample_text = [tokenizer.decode(element, skip_special_tokens=False) for element in input_ids[wrong_sample_index]]\n",
    "  wrong_sample_label = [num_to_label_dict[lab] for lab in list(valid_labels[wrong_sample_index])]\n",
    "  wrong_sample_pred = [num_to_label_dict[pred] for pred in list(valid_predict[wrong_sample_index])]\n",
    "  wrong_sample_output = valid_output[wrong_sample_index].tolist()\n",
    "\n",
    "  diff_prob, same_prob = [], []\n",
    "  for element in wrong_sample_output:\n",
    "      diff_prob.append(element[0])\n",
    "      same_prob.append(element[1])\n",
    "\n",
    "  return wrong_sample_text, wrong_sample_label, wrong_sample_pred, diff_prob, same_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21bdf18",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca73cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamP(Optimizer):\n",
    "  def __init__(\n",
    "      self,\n",
    "      params,\n",
    "      lr=1e-3,\n",
    "      betas=(0.9, 0.999),\n",
    "      eps=1e-8,\n",
    "      weight_decay=0,\n",
    "      delta=0.1,\n",
    "      wd_ratio=0.1,\n",
    "      nesterov=False,\n",
    "      ):\n",
    "    defaults = dict(\n",
    "        lr=lr,\n",
    "        betas=betas,\n",
    "        eps=eps,\n",
    "        weight_decay=weight_decay,\n",
    "        delta=delta,\n",
    "        wd_ratio=wd_ratio,\n",
    "        nesterov=nesterov,\n",
    "        )\n",
    "    super(AdamP, self).__init__(params, defaults)\n",
    "\n",
    "  def _channel_view(self, x):\n",
    "    return x.view(x.size(0), -1)\n",
    "\n",
    "  def _layer_view(self, x):\n",
    "    return x.view(1, -1)\n",
    "\n",
    "  def _cosine_similarity(self, x, y, eps, view_func):\n",
    "    x = view_func(x)\n",
    "    y = view_func(y)\n",
    "\n",
    "    return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n",
    "\n",
    "  def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n",
    "    wd = 1\n",
    "    expand_size = [-1] + [1] * (len(p.shape) - 1)\n",
    "    for view_func in [self._channel_view, self._layer_view]:\n",
    "      \n",
    "      cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n",
    "      \n",
    "      if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
    "        p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n",
    "        perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n",
    "        wd = wd_ratio\n",
    "        return perturb, wd\n",
    "\n",
    "    return perturb, wd\n",
    "\n",
    "  def step(self, closure=None):\n",
    "    loss = None\n",
    "    if closure is not None:\n",
    "      loss = closure()\n",
    "\n",
    "    for group in self.param_groups:\n",
    "      for p in group[\"params\"]:\n",
    "        if p.grad is None:\n",
    "          continue\n",
    "\n",
    "        grad = p.grad.data\n",
    "        beta1, beta2 = group[\"betas\"]\n",
    "        nesterov = group[\"nesterov\"]\n",
    "\n",
    "        state = self.state[p]\n",
    "\n",
    "        # State initialization\n",
    "        if len(state) == 0:\n",
    "          state[\"step\"] = 0\n",
    "          state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "          state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "        # Adam\n",
    "        exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "\n",
    "        state[\"step\"] += 1\n",
    "        bias_correction1 = 1 - beta1 ** state[\"step\"]\n",
    "        bias_correction2 = 1 - beta2 ** state[\"step\"]\n",
    "\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "        denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(\n",
    "            group[\"eps\"]\n",
    "            )\n",
    "        step_size = group[\"lr\"] / bias_correction1\n",
    "\n",
    "        if nesterov:\n",
    "          perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\n",
    "        else:\n",
    "          perturb = exp_avg / denom\n",
    "\n",
    "        # Projection\n",
    "        wd_ratio = 1\n",
    "        if len(p.shape) > 1:\n",
    "          perturb, wd_ratio = self._projection(\n",
    "              p,\n",
    "              grad,\n",
    "              perturb,\n",
    "              group[\"delta\"],\n",
    "              group[\"wd_ratio\"],\n",
    "              group[\"eps\"],\n",
    "              )\n",
    "\n",
    "          # Weight decay\n",
    "        if group[\"weight_decay\"] > 0:\n",
    "          p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"] * wd_ratio)\n",
    "\n",
    "          # Step\n",
    "        p.data.add_(perturb, alpha=-step_size)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def get_optimizer(model, args):\n",
    "  if args.optimizer == \"Adam\":\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
    "  elif args.optimizer == \"AdamW\":\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
    "  elif args.optimizer == \"AdamP\":\n",
    "    optimizer = AdamP(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=0.01,\n",
    "        delta=0.1,\n",
    "        wd_ratio=0.1,\n",
    "        nesterov=False,\n",
    "        )\n",
    "  else:\n",
    "    raise NotImplementedError('Optimizer not available')\n",
    "\n",
    "  # 모든 parameter들의 grad값을 0으로 초기화\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ec38a9",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6500dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
    "  \"\"\"\n",
    "    optimizer (Optimizer): Wrapped optimizer.\n",
    "    first_cycle_steps (int): First cycle step size.\n",
    "    cycle_mult(float): Cycle steps magnification. Default: -1.\n",
    "    max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
    "    min_lr(float): Min learning rate. Default: 0.001.\n",
    "    warmup_steps(int): Linear warmup step size. Default: 0.\n",
    "    gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
    "    last_epoch (int): The index of last epoch. Default: -1.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               optimizer : torch.optim.Optimizer,\n",
    "               first_cycle_steps : int,\n",
    "               cycle_mult : float = 1.,\n",
    "               max_lr : float = 0.1,\n",
    "               min_lr : float = 0.001,\n",
    "               warmup_steps : int = 0,\n",
    "               gamma : float = 1.,\n",
    "               last_epoch : int = -1\n",
    "               ):\n",
    "    assert warmup_steps < first_cycle_steps\n",
    "        \n",
    "    self.first_cycle_steps = first_cycle_steps # first cycle step size\n",
    "    self.cycle_mult = cycle_mult # cycle steps magnification\n",
    "    self.base_max_lr = max_lr # first max learning rate\n",
    "    self.max_lr = max_lr # max learning rate in the current cycle\n",
    "    self.min_lr = min_lr # min learning rate\n",
    "    self.warmup_steps = warmup_steps # warmup step size\n",
    "    self.gamma = gamma # decrease rate of max learning rate by cycle\n",
    "    \n",
    "    self.cur_cycle_steps = first_cycle_steps # first cycle step size\n",
    "    self.cycle = 0 # cycle count\n",
    "    self.step_in_cycle = last_epoch # step size of the current cycle\n",
    "    \n",
    "    super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "    # set learning rate min_lr\n",
    "    self.init_lr()\n",
    "    \n",
    "  def init_lr(self):\n",
    "    self.base_lrs = []\n",
    "    for param_group in self.optimizer.param_groups:\n",
    "      param_group['lr'] = self.min_lr\n",
    "      self.base_lrs.append(self.min_lr)\n",
    "    \n",
    "  def get_lr(self):\n",
    "    if self.step_in_cycle == -1:\n",
    "      return self.base_lrs\n",
    "    elif self.step_in_cycle < self.warmup_steps:\n",
    "      return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
    "    else:\n",
    "      return [base_lr + (self.max_lr - base_lr) \\\n",
    "              * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
    "                              / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
    "              for base_lr in self.base_lrs]\n",
    "\n",
    "  def step(self, epoch=None):\n",
    "    if epoch is None:\n",
    "      epoch = self.last_epoch + 1\n",
    "      self.step_in_cycle = self.step_in_cycle + 1\n",
    "      if self.step_in_cycle >= self.cur_cycle_steps:\n",
    "        self.cycle += 1\n",
    "        self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
    "        self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
    "      else:\n",
    "        if epoch >= self.first_cycle_steps:\n",
    "          if self.cycle_mult == 1.:\n",
    "            self.step_in_cycle = epoch % self.first_cycle_steps\n",
    "            self.cycle = epoch // self.first_cycle_steps\n",
    "          else:\n",
    "            n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
    "            self.cycle = n\n",
    "            self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
    "            self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
    "        else:\n",
    "          self.cur_cycle_steps = self.first_cycle_steps\n",
    "          self.step_in_cycle = epoch\n",
    "                \n",
    "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, args, total_batch_):\n",
    "  if args.scheduler == \"plateau\":\n",
    "      scheduler = ReduceLROnPlateau(\n",
    "          optimizer, patience=2, factor=0.85, mode=\"max\", verbose=True\n",
    "      )\n",
    "  elif args.scheduler == \"linear\":\n",
    "      scheduler = get_linear_schedule_with_warmup(\n",
    "          optimizer,\n",
    "          # num_warmup_steps=int(total_batch_*args.epochs*0.1),\n",
    "          num_warmup_steps=args.warmup_steps,\n",
    "          num_training_steps=int(total_batch_*args.epochs),\n",
    "      )\n",
    "  elif args.scheduler == \"cosine\":\n",
    "      scheduler = CosineAnnealingWarmupRestarts(  \n",
    "          optimizer,\n",
    "          first_cycle_steps=200,\n",
    "          warmup_steps=args.warmup_steps,\n",
    "          cycle_mult=args.cycle_mult,\n",
    "          max_lr=args.lr,\n",
    "          min_lr=args.lr * 0.01,\n",
    "          gamma=0.9,\n",
    "      )\n",
    "  else:\n",
    "    raise NotImplementedError('LR Scheduler not available')\n",
    "\n",
    "  return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91769c",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f937044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "  def __init__(self, weight=None,\n",
    "               gamma=2., reduction='mean'):\n",
    "    nn.Module.__init__(self)\n",
    "    self.weight = weight\n",
    "    self.gamma = gamma\n",
    "    self.reduction = reduction\n",
    "\n",
    "  def forward(self, input_tensor, target_tensor):\n",
    "    log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "    prob = torch.exp(log_prob)\n",
    "    return F.nll_loss(\n",
    "        ((1 - prob) ** self.gamma) * log_prob,\n",
    "        target_tensor,\n",
    "        weight=self.weight,\n",
    "        reduction=self.reduction\n",
    "        )\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "  def __init__(self, classes=3, smoothing=0.0, dim=-1):\n",
    "    super(LabelSmoothingLoss, self).__init__()\n",
    "    self.confidence = 1.0 - smoothing\n",
    "    self.smoothing = smoothing\n",
    "    self.cls = classes\n",
    "    self.dim = dim\n",
    "\n",
    "  def forward(self, pred, target):\n",
    "    pred = pred.log_softmax(dim=self.dim)\n",
    "    with torch.no_grad():\n",
    "      true_dist = torch.zeros_like(pred)\n",
    "      true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "      true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "    return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "\n",
    "def get_criterion(args):\n",
    "  if args.smoothing!=0 and args.criterion == 'smoothing':\n",
    "    criterion = LabelSmoothingLoss(smoothing=args.smoothing)\n",
    "  elif args.criterion == 'cross':\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "  elif args.criterion == 'focal':\n",
    "    criterion = FocalLoss(gamma=2.0)\n",
    "  else:\n",
    "    raise NotImplementedError('Criterion not available')\n",
    "  return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3855d3d",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbd790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 folds를 위해 나누어져있는 데이터를 다시 합쳤습니다.\n",
    "tdataset = load_dataset(\"csv\", data_files='/content/drive/MyDrive/train_data_lv1.csv')['train']\n",
    "vdataset = load_dataset(\"csv\", data_files='/content/drive/MyDrive/valid_data_lv1.csv')['train']\n",
    "rawdataset = concatenate_datasets([tdataset, vdataset])\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "tokenizer.truncation_side = 'left'\n",
    "\n",
    "def example_fn(examples):\n",
    "    outputs = tokenizer(examples['code1'], examples['code2'], padding='max_length', max_length=512, truncation=True)\n",
    "    outputs['labels'] = examples['similar']\n",
    "    return outputs\n",
    "\n",
    "dataset = rawdataset.map(example_fn, remove_columns=['code1', 'code2', 'similar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a8edb",
   "metadata": {},
   "source": [
    "# Arguments 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'current device : {device}')\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"seed\":42,\n",
    "    \"optimizer\":\"AdamW\",\n",
    "    \"scheduler\":\"linear\",\n",
    "    \"warmup_steps\":500,\n",
    "    \"cycle_mult\":1.2,\n",
    "    \"batch_size\": 16,\n",
    "    \"patience\":5,\n",
    "    \"n_splits\":6,\n",
    "    \"epochs\":3,\n",
    "    \"lr\": 2e-05,\n",
    "    \"criterion\":'cross',\n",
    "    \"smoothing\": 0.0,\n",
    "    \"model\": \"microsoft/graphcodebert-base\",\n",
    "    \"logging_wrong_samples\":True,\n",
    "    })\n",
    "\n",
    "project_name = \"graphcodebert_Bs16_OptAdamW_ScduLinear_Sm0.0\"\n",
    "args.update(\n",
    "            {\n",
    "                \"project_name\":project_name,\n",
    "                \"model_name\":project_name,\n",
    "             }\n",
    "            )\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a075ab",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07119169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52524bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = get_criterion(args)\n",
    "config =  AutoConfig.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "config.num_labels = 2\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"microsoft/graphcodebert-base\", config=config)\n",
    "model.to(device)\n",
    "\n",
    "best_val_acc_list = []\n",
    "gap = int(len(dataset) / args.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed41da",
   "metadata": {},
   "source": [
    "## fold 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ccca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab 런타임문제로 cell 단위로 나누어 진행했습니다.\n",
    "f = 1\n",
    "\n",
    "print(f\"---------------------------------- {f} fold----------------------------------\")\n",
    "\n",
    "run = wandb.init(project=args.project_name)\n",
    "wandb.run.name = f'{args.model_name}/{f}-fold'\n",
    "wandb.config.update(args)\n",
    "os.makedirs(f'./models/{args.model_name}/{f}-fold', exist_ok=True)\n",
    "\n",
    "total_size = len(dataset)\n",
    "total_ids = list(range(total_size))\n",
    "del_ids = list(range((f-1)*gap, f*gap))\n",
    "training_ids = set(total_ids) - set(del_ids)\n",
    "\n",
    "training_dset = dataset.select(list(training_ids))\n",
    "eval_dset = dataset.select(del_ids)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainloader = DataLoader(training_dset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=True,\n",
    "                          collate_fn = collator\n",
    "                          )\n",
    "\n",
    "validloader = DataLoader(eval_dset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=False,\n",
    "                          collate_fn = collator\n",
    "                          )\n",
    "\n",
    "total_batch_ = len(trainloader)\n",
    "valid_batch_ = len(validloader)\n",
    "\n",
    "optimizer = get_optimizer(model, args)\n",
    "scheduler = get_scheduler(optimizer, args, total_batch_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631fd135",
   "metadata": {},
   "source": [
    "## epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 1\n",
    "e = 1\n",
    "\n",
    "print(f\"------------------------------ {f} fold {e} epoch------------------------------\")\n",
    "\n",
    "model.train()\n",
    "epoch_perform, batch_perform = np.zeros(2), np.zeros(2)\n",
    "print()\t\n",
    "progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), leave=True, position=0,)\n",
    "for j, v in progress_bar:\n",
    "  input_ids, attention_mask, labels = v['input_ids'].to(device), v['attention_mask'].to(device), v['labels'].to(device)\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  outputs = model(input_ids, attention_mask) ## label을 안 넣어서 logits값만 출력\n",
    "  output = outputs.logits # The outputs object is a SequenceClassifierOutput\n",
    "  loss = criterion(output, labels)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  scheduler.step()\n",
    "  for learning_rate in scheduler.get_lr():\n",
    "    wandb.log({\"learning_rate\": learning_rate})\n",
    "\n",
    "  predict = output.argmax(dim=-1)\n",
    "  predict = predict.detach().cpu().numpy()\n",
    "  labels = labels.detach().cpu().numpy()\n",
    "  acc = accuracy_score(labels, predict)\n",
    "\n",
    "  batch_perform += np.array([loss.item(), acc])\n",
    "  epoch_perform += np.array([loss.item(), acc])\n",
    "\n",
    "  if (j + 1) % 50 == 0:\n",
    "    print(\n",
    "        f\"Epoch {e} #{j + 1} -- loss: {batch_perform[0] / 50}, acc: {batch_perform[1] / 50}\"\n",
    "    )\n",
    "    batch_perform = np.zeros(2)\n",
    "print()\n",
    "print(\n",
    "    f\"Epoch {e} loss: {epoch_perform[0] / total_batch_}, acc: {epoch_perform[1] / total_batch_}\"\n",
    "    )\n",
    "wandb.log({\n",
    "    \"epoch\": e,\n",
    "    \"Train epoch Loss\": epoch_perform[0] / total_batch_,\n",
    "    \"Train epoch Acc\": epoch_perform[1] / total_batch_}\n",
    "    )\n",
    "torch.save(model.state_dict(), f\"./models/{args.model_name}/{f}-fold/train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fd390",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 1\n",
    "e = 1\n",
    "best_val_loss, best_val_acc, = np.inf, 0\n",
    "# Validation\n",
    "load_path = f'./models/{args.model_name}/{f}-fold/train.pt'\n",
    "model.load_state_dict(torch.load(load_path,map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "valid_perform = np.zeros(2)\n",
    "\n",
    "all_valid_predict_lst = []\n",
    "all_valid_labels_lst = []\n",
    "\n",
    "# wandb에 잘못 분류하는 코드 Pair 기록\n",
    "wrong_sample_dict = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for v in validloader:\n",
    "      input_ids, attention_mask, valid_labels = v[\"input_ids\"].to(device), v[\"attention_mask\"].to(device), v[\"labels\"].to(device)\n",
    "      \n",
    "      valid_outputs = model(input_ids, attention_mask)\n",
    "      valid_output = valid_outputs.logits\n",
    "      valid_loss = criterion(valid_output, valid_labels)\n",
    "      \n",
    "      valid_predict = valid_output.argmax(dim=-1)\n",
    "      valid_predict = valid_predict.detach().cpu().numpy()\n",
    "      valid_labels = valid_labels.detach().cpu().numpy()\n",
    "\n",
    "      \n",
    "      # wandb에 잘못 분류하는 코드 Pair 기록\n",
    "      if args.logging_wrong_samples:\n",
    "        wrong_sample_index = np.where(valid_labels!=valid_predict)[0]\n",
    "        if len(wrong_sample_index)>0:\n",
    "          wrong_sample_text, wrong_sample_label, wrong_sample_pred, entailment_prob, contradiction_prob = wrong_batch_for_wandb(tokenizer, wrong_sample_index, input_ids, valid_labels, valid_predict, valid_output)\n",
    "\n",
    "          wrong_sample_dict['입력 코드 Pair'] += wrong_sample_text\n",
    "          wrong_sample_dict['실제값'] += wrong_sample_label\n",
    "          wrong_sample_dict['예측값'] += wrong_sample_pred\n",
    "          wrong_sample_dict['diff_logit'] += entailment_prob\n",
    "          wrong_sample_dict['same_logit'] += contradiction_prob\n",
    "\n",
    "\n",
    "      valid_acc = accuracy_score(valid_labels, valid_predict)\n",
    "      valid_perform += np.array([valid_loss.item(), valid_acc])\n",
    "\n",
    "      all_valid_predict_lst += list(valid_predict)\n",
    "      all_valid_labels_lst += list(valid_labels)\n",
    "\n",
    "# Model 저장\n",
    "\n",
    "val_total_loss = valid_perform[0] / valid_batch_\n",
    "val_total_acc = valid_perform[1] / valid_batch_\n",
    "best_val_loss = min(best_val_loss, val_total_loss)\n",
    "\n",
    "\n",
    "if val_total_acc > best_val_acc:\n",
    "    print(f\"New best model for val accuracy : {val_total_acc}! saving the best model..\")\n",
    "    torch.save(model.state_dict(), f\"./models/{args.model_name}/{f}-fold/best.pt\")\n",
    "\n",
    "    best_val_acc = val_total_acc\n",
    "\n",
    "    # wandb에 Confusion Matrix 생성\n",
    "    class_names = ['diff','same'] # (0,1)\n",
    "    wandb.log({f\"{e}_epoch_conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                      y_true=all_valid_labels_lst, preds=all_valid_predict_lst,\n",
    "                                                                      class_names=class_names)})\n",
    "      \n",
    "    if args.logging_wrong_samples and val_total_acc > 0.91:\n",
    "\n",
    "      wrong_sample_df = pd.DataFrame(wrong_sample_dict)\n",
    "      wrong_sample_df.to_csv(f\"./models/{args.model_name}/{f}-fold/wrong_df.csv\",index=False)\n",
    "      print('='*15,f'Fold{f} Wrong DataFrame Saved','='*15)\n",
    "      text_table = wandb.Table(data = wrong_sample_df)\n",
    "      run.log({f\"{f}_fold_wrong_samples\" : text_table})\n",
    "    \n",
    "print()\n",
    "print(\n",
    "    f\">>>> Validation loss: {val_total_loss}, Acc: {val_total_acc}\"\n",
    "    )\n",
    "print()\n",
    "wandb.log({\n",
    "    \"epoch\": e,\n",
    "    \"Last_Valid Loss\": val_total_loss,\n",
    "    \"Last_Valid Acc\": val_total_acc,\n",
    "    })\n",
    "best_val_acc_list.append(best_val_acc)\n",
    "print('='*50)\n",
    "print(f\"{f}fold best_val_acc_list : {best_val_acc_list}\")\n",
    "print('='*15, f'{f}fold Final Score(ACC) : {np.mean(best_val_acc_list)}', '='*15)\n",
    "wandb.log({\n",
    "f\"Total Mean ACC ({f}fold)\": np.mean(best_val_acc_list)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2470570b",
   "metadata": {},
   "source": [
    "## fold 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab 런타임문제로 cell 단위로 나누어 진행했습니다.\n",
    "f = 2\n",
    "\n",
    "print(f\"---------------------------------- {f} fold----------------------------------\")\n",
    "\n",
    "run = wandb.init(project=args.project_name)\n",
    "wandb.run.name = f'{args.model_name}/{f}-fold'\n",
    "wandb.config.update(args)\n",
    "os.makedirs(f'./models/{args.model_name}/{f}-fold', exist_ok=True)\n",
    "\n",
    "total_size = len(dataset)\n",
    "total_ids = list(range(total_size))\n",
    "del_ids = list(range((f-1)*gap, f*gap))\n",
    "training_ids = set(total_ids) - set(del_ids)\n",
    "\n",
    "training_dset = dataset.select(list(training_ids))\n",
    "eval_dset = dataset.select(del_ids)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainloader = DataLoader(training_dset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=True,\n",
    "                          collate_fn = collator\n",
    "                          )\n",
    "\n",
    "validloader = DataLoader(eval_dset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=False,\n",
    "                          collate_fn = collator\n",
    "                          )\n",
    "\n",
    "total_batch_ = len(trainloader)\n",
    "valid_batch_ = len(validloader)\n",
    "\n",
    "optimizer = get_optimizer(model, args)\n",
    "scheduler = get_scheduler(optimizer, args, total_batch_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c00d23",
   "metadata": {},
   "source": [
    "## epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 2\n",
    "e = 1\n",
    "\n",
    "print(f\"------------------------------ {f} fold {e} epoch------------------------------\")\n",
    "\n",
    "model.train()\n",
    "epoch_perform, batch_perform = np.zeros(2), np.zeros(2)\n",
    "print()\t\n",
    "progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), leave=True, position=0,)\n",
    "for j, v in progress_bar:\n",
    "  input_ids, attention_mask, labels = v['input_ids'].to(device), v['attention_mask'].to(device), v['labels'].to(device)\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  outputs = model(input_ids, attention_mask) ## label을 안 넣어서 logits값만 출력\n",
    "  output = outputs.logits # The outputs object is a SequenceClassifierOutput\n",
    "  loss = criterion(output, labels)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  scheduler.step()\n",
    "  for learning_rate in scheduler.get_lr():\n",
    "    wandb.log({\"learning_rate\": learning_rate})\n",
    "\n",
    "  predict = output.argmax(dim=-1)\n",
    "  predict = predict.detach().cpu().numpy()\n",
    "  labels = labels.detach().cpu().numpy()\n",
    "  acc = accuracy_score(labels, predict)\n",
    "\n",
    "  batch_perform += np.array([loss.item(), acc])\n",
    "  epoch_perform += np.array([loss.item(), acc])\n",
    "\n",
    "  if (j + 1) % 50 == 0:\n",
    "    print(\n",
    "        f\"Epoch {e} #{j + 1} -- loss: {batch_perform[0] / 50}, acc: {batch_perform[1] / 50}\"\n",
    "    )\n",
    "    batch_perform = np.zeros(2)\n",
    "print()\n",
    "print(\n",
    "    f\"Epoch {e} loss: {epoch_perform[0] / total_batch_}, acc: {epoch_perform[1] / total_batch_}\"\n",
    "    )\n",
    "wandb.log({\n",
    "    \"epoch\": e,\n",
    "    \"Train epoch Loss\": epoch_perform[0] / total_batch_,\n",
    "    \"Train epoch Acc\": epoch_perform[1] / total_batch_}\n",
    "    )\n",
    "torch.save(model.state_dict(), f\"./models/{args.model_name}/{f}-fold/train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703535eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 2\n",
    "e = 1\n",
    "best_val_loss, best_val_acc, = np.inf, 0\n",
    "# Validation\n",
    "load_path = f'./models/{args.model_name}/{f}-fold/train.pt'\n",
    "model.load_state_dict(torch.load(load_path,map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "valid_perform = np.zeros(2)\n",
    "\n",
    "all_valid_predict_lst = []\n",
    "all_valid_labels_lst = []\n",
    "\n",
    "# wandb에 잘못 분류하는 코드 Pair 기록\n",
    "wrong_sample_dict = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for v in validloader:\n",
    "      input_ids, attention_mask, valid_labels = v[\"input_ids\"].to(device), v[\"attention_mask\"].to(device), v[\"labels\"].to(device)\n",
    "      \n",
    "      valid_outputs = model(input_ids, attention_mask)\n",
    "      valid_output = valid_outputs.logits\n",
    "      valid_loss = criterion(valid_output, valid_labels)\n",
    "      \n",
    "      valid_predict = valid_output.argmax(dim=-1)\n",
    "      valid_predict = valid_predict.detach().cpu().numpy()\n",
    "      valid_labels = valid_labels.detach().cpu().numpy()\n",
    "\n",
    "      \n",
    "      # wandb에 잘못 분류하는 코드 Pair 기록\n",
    "      if args.logging_wrong_samples:\n",
    "        wrong_sample_index = np.where(valid_labels!=valid_predict)[0]\n",
    "        if len(wrong_sample_index)>0:\n",
    "          wrong_sample_text, wrong_sample_label, wrong_sample_pred, entailment_prob, contradiction_prob = wrong_batch_for_wandb(tokenizer, wrong_sample_index, input_ids, valid_labels, valid_predict, valid_output)\n",
    "\n",
    "          wrong_sample_dict['입력 코드 Pair'] += wrong_sample_text\n",
    "          wrong_sample_dict['실제값'] += wrong_sample_label\n",
    "          wrong_sample_dict['예측값'] += wrong_sample_pred\n",
    "          wrong_sample_dict['diff_logit'] += entailment_prob\n",
    "          wrong_sample_dict['same_logit'] += contradiction_prob\n",
    "\n",
    "\n",
    "      valid_acc = accuracy_score(valid_labels, valid_predict)\n",
    "      valid_perform += np.array([valid_loss.item(), valid_acc])\n",
    "\n",
    "      all_valid_predict_lst += list(valid_predict)\n",
    "      all_valid_labels_lst += list(valid_labels)\n",
    "\n",
    "# Model 저장\n",
    "\n",
    "val_total_loss = valid_perform[0] / valid_batch_\n",
    "val_total_acc = valid_perform[1] / valid_batch_\n",
    "best_val_loss = min(best_val_loss, val_total_loss)\n",
    "\n",
    "\n",
    "if val_total_acc > best_val_acc:\n",
    "    print(f\"New best model for val accuracy : {val_total_acc}! saving the best model..\")\n",
    "    torch.save(model.state_dict(), f\"./models/{args.model_name}/{f}-fold/best.pt\")\n",
    "\n",
    "    best_val_acc = val_total_acc\n",
    "\n",
    "    # wandb에 Confusion Matrix 생성\n",
    "    class_names = ['diff','same'] # (0,1)\n",
    "    wandb.log({f\"{e}_epoch_conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                      y_true=all_valid_labels_lst, preds=all_valid_predict_lst,\n",
    "                                                                      class_names=class_names)})\n",
    "      \n",
    "    if args.logging_wrong_samples and val_total_acc > 0.91:\n",
    "\n",
    "      wrong_sample_df = pd.DataFrame(wrong_sample_dict)\n",
    "      wrong_sample_df.to_csv(f\"./models/{args.model_name}/{f}-fold/wrong_df.csv\",index=False)\n",
    "      print('='*15,f'Fold{f} Wrong DataFrame Saved','='*15)\n",
    "      text_table = wandb.Table(data = wrong_sample_df)\n",
    "      run.log({f\"{f}_fold_wrong_samples\" : text_table})\n",
    "    \n",
    "print()\n",
    "print(\n",
    "    f\">>>> Validation loss: {val_total_loss}, Acc: {val_total_acc}\"\n",
    "    )\n",
    "print()\n",
    "wandb.log({\n",
    "    \"epoch\": e,\n",
    "    \"Last_Valid Loss\": val_total_loss,\n",
    "    \"Last_Valid Acc\": val_total_acc,\n",
    "    })\n",
    "best_val_acc_list.append(best_val_acc)\n",
    "print('='*50)\n",
    "print(f\"{f}fold best_val_acc_list : {best_val_acc_list}\")\n",
    "print('='*15, f'{f}fold Final Score(ACC) : {np.mean(best_val_acc_list)}', '='*15)\n",
    "wandb.log({\n",
    "f\"Total Mean ACC ({f}fold)\": np.mean(best_val_acc_list)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6fb68",
   "metadata": {},
   "source": [
    "## fold 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26272a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab 런타임문제로 cell 단위로 나누어 진행했습니다.\n",
    "f = 3\n",
    "\n",
    "print(f\"---------------------------------- {f} fold----------------------------------\")\n",
    "\n",
    "run = wandb.init(project=args.project_name)\n",
    "wandb.run.name = f'{args.model_name}/{f}-fold'\n",
    "wandb.config.update(args)\n",
    "os.makedirs(f'./models/{args.model_name}/{f}-fold', exist_ok=True)\n",
    "\n",
    "total_size = len(dataset)\n",
    "total_ids = list(range(total_size))\n",
    "del_ids = list(range((f-1)*gap, f*gap))\n",
    "training_ids = set(total_ids) - set(del_ids)\n",
    "\n",
    "training_dset = dataset.select(list(training_ids))\n",
    "eval_dset = dataset.select(del_ids)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainloader = DataLoader(training_dset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=True,\n",
    "                          collate_fn = collator\n",
    "                          )\n",
    "\n",
    "validloader = DataLoader(eval_dset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=False,\n",
    "                          collate_fn = collator\n",
    "                          )\n",
    "\n",
    "total_batch_ = len(trainloader)\n",
    "valid_batch_ = len(validloader)\n",
    "\n",
    "optimizer = get_optimizer(model, args)\n",
    "scheduler = get_scheduler(optimizer, args, total_batch_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c494a2",
   "metadata": {},
   "source": [
    "## epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a89f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 3\n",
    "e = 1\n",
    "\n",
    "print(f\"------------------------------ {f} fold {e} epoch------------------------------\")\n",
    "\n",
    "model.train()\n",
    "epoch_perform, batch_perform = np.zeros(2), np.zeros(2)\n",
    "print()\t\n",
    "progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), leave=True, position=0,)\n",
    "for j, v in progress_bar:\n",
    "  input_ids, attention_mask, labels = v['input_ids'].to(device), v['attention_mask'].to(device), v['labels'].to(device)\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  outputs = model(input_ids, attention_mask) ## label을 안 넣어서 logits값만 출력\n",
    "  output = outputs.logits # The outputs object is a SequenceClassifierOutput\n",
    "  loss = criterion(output, labels)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  scheduler.step()\n",
    "  for learning_rate in scheduler.get_lr():\n",
    "    wandb.log({\"learning_rate\": learning_rate})\n",
    "\n",
    "  predict = output.argmax(dim=-1)\n",
    "  predict = predict.detach().cpu().numpy()\n",
    "  labels = labels.detach().cpu().numpy()\n",
    "  acc = accuracy_score(labels, predict)\n",
    "\n",
    "  batch_perform += np.array([loss.item(), acc])\n",
    "  epoch_perform += np.array([loss.item(), acc])\n",
    "\n",
    "  if (j + 1) % 50 == 0:\n",
    "    print(\n",
    "        f\"Epoch {e} #{j + 1} -- loss: {batch_perform[0] / 50}, acc: {batch_perform[1] / 50}\"\n",
    "    )\n",
    "    batch_perform = np.zeros(2)\n",
    "print()\n",
    "print(\n",
    "    f\"Epoch {e} loss: {epoch_perform[0] / total_batch_}, acc: {epoch_perform[1] / total_batch_}\"\n",
    "    )\n",
    "wandb.log({\n",
    "    \"epoch\": e,\n",
    "    \"Train epoch Loss\": epoch_perform[0] / total_batch_,\n",
    "    \"Train epoch Acc\": epoch_perform[1] / total_batch_}\n",
    "    )\n",
    "torch.save(model.state_dict(), f\"./models/{args.model_name}/{f}-fold/train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 3\n",
    "e = 1\n",
    "best_val_loss, best_val_acc, = np.inf, 0\n",
    "# Validation\n",
    "load_path = f'./models/{args.model_name}/{f}-fold/train.pt'\n",
    "model.load_state_dict(torch.load(load_path,map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "valid_perform = np.zeros(2)\n",
    "\n",
    "all_valid_predict_lst = []\n",
    "all_valid_labels_lst = []\n",
    "\n",
    "# wandb에 잘못 분류하는 코드 Pair 기록\n",
    "wrong_sample_dict = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for v in validloader:\n",
    "      input_ids, attention_mask, valid_labels = v[\"input_ids\"].to(device), v[\"attention_mask\"].to(device), v[\"labels\"].to(device)\n",
    "      \n",
    "      valid_outputs = model(input_ids, attention_mask)\n",
    "      valid_output = valid_outputs.logits\n",
    "      valid_loss = criterion(valid_output, valid_labels)\n",
    "      \n",
    "      valid_predict = valid_output.argmax(dim=-1)\n",
    "      valid_predict = valid_predict.detach().cpu().numpy()\n",
    "      valid_labels = valid_labels.detach().cpu().numpy()\n",
    "\n",
    "      \n",
    "      # wandb에 잘못 분류하는 코드 Pair 기록\n",
    "      if args.logging_wrong_samples:\n",
    "        wrong_sample_index = np.where(valid_labels!=valid_predict)[0]\n",
    "        if len(wrong_sample_index)>0:\n",
    "          wrong_sample_text, wrong_sample_label, wrong_sample_pred, entailment_prob, contradiction_prob = wrong_batch_for_wandb(tokenizer, wrong_sample_index, input_ids, valid_labels, valid_predict, valid_output)\n",
    "\n",
    "          wrong_sample_dict['입력 코드 Pair'] += wrong_sample_text\n",
    "          wrong_sample_dict['실제값'] += wrong_sample_label\n",
    "          wrong_sample_dict['예측값'] += wrong_sample_pred\n",
    "          wrong_sample_dict['diff_logit'] += entailment_prob\n",
    "          wrong_sample_dict['same_logit'] += contradiction_prob\n",
    "\n",
    "\n",
    "      valid_acc = accuracy_score(valid_labels, valid_predict)\n",
    "      valid_perform += np.array([valid_loss.item(), valid_acc])\n",
    "\n",
    "      all_valid_predict_lst += list(valid_predict)\n",
    "      all_valid_labels_lst += list(valid_labels)\n",
    "\n",
    "# Model 저장\n",
    "\n",
    "val_total_loss = valid_perform[0] / valid_batch_\n",
    "val_total_acc = valid_perform[1] / valid_batch_\n",
    "best_val_loss = min(best_val_loss, val_total_loss)\n",
    "\n",
    "\n",
    "if val_total_acc > best_val_acc:\n",
    "    print(f\"New best model for val accuracy : {val_total_acc}! saving the best model..\")\n",
    "    torch.save(model.state_dict(), f\"./models/{args.model_name}/{f}-fold/best.pt\")\n",
    "\n",
    "    best_val_acc = val_total_acc\n",
    "\n",
    "    # wandb에 Confusion Matrix 생성\n",
    "    class_names = ['diff','same'] # (0,1)\n",
    "    wandb.log({f\"{e}_epoch_conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                      y_true=all_valid_labels_lst, preds=all_valid_predict_lst,\n",
    "                                                                      class_names=class_names)})\n",
    "      \n",
    "    if args.logging_wrong_samples and val_total_acc > 0.91:\n",
    "\n",
    "      wrong_sample_df = pd.DataFrame(wrong_sample_dict)\n",
    "      wrong_sample_df.to_csv(f\"./models/{args.model_name}/{f}-fold/wrong_df.csv\",index=False)\n",
    "      print('='*15,f'Fold{f} Wrong DataFrame Saved','='*15)\n",
    "      text_table = wandb.Table(data = wrong_sample_df)\n",
    "      run.log({f\"{f}_fold_wrong_samples\" : text_table})\n",
    "    \n",
    "print()\n",
    "print(\n",
    "    f\">>>> Validation loss: {val_total_loss}, Acc: {val_total_acc}\"\n",
    "    )\n",
    "print()\n",
    "wandb.log({\n",
    "    \"epoch\": e,\n",
    "    \"Last_Valid Loss\": val_total_loss,\n",
    "    \"Last_Valid Acc\": val_total_acc,\n",
    "    })\n",
    "best_val_acc_list.append(best_val_acc)\n",
    "print('='*50)\n",
    "print(f\"{f}fold best_val_acc_list : {best_val_acc_list}\")\n",
    "print('='*15, f'{f}fold Final Score(ACC) : {np.mean(best_val_acc_list)}', '='*15)\n",
    "wandb.log({\n",
    "f\"Total Mean ACC ({f}fold)\": np.mean(best_val_acc_list)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611f109",
   "metadata": {},
   "source": [
    "## fold 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d7356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab 런타임문제로 cell 단위로 나누어 진행했습니다.\n",
    "f = 4\n",
    "\n",
    "print(f\"---------------------------------- {f} fold----------------------------------\")\n",
    "\n",
    "run = wandb.init(project=args.project_name)\n",
    "wandb.run.name = f'{args.model_name}/{f}-fold'\n",
    "wandb.config.update(args)\n",
    "os.makedirs(f'./models/{args.model_name}/{f}-fold', exist_ok=True)\n",
    "\n",
    "total_size = len(dataset)\n",
    "total_ids = list(range(total_size))\n",
    "del_ids = list(range((f-1)*gap, f*gap))\n",
    "training_ids = set(total_ids) - set(del_ids)\n",
    "\n",
    "training_dset = dataset.select(list(training_ids))\n",
    "eval_dset = dataset.select(del_ids)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainloader = DataLoader(training_dset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=True,\n",
    "                          collate_fn = collator\n",
    "                          )\n",
    "\n",
    "validloader = DataLoader(eval_dset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=False,\n",
    "                          collate_fn = collator\n",
    "                          )\n",
    "\n",
    "total_batch_ = len(trainloader)\n",
    "valid_batch_ = len(validloader)\n",
    "\n",
    "optimizer = get_optimizer(model, args)\n",
    "scheduler = get_scheduler(optimizer, args, total_batch_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea866daa",
   "metadata": {},
   "source": [
    "## epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a700574",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 4\n",
    "e = 1\n",
    "\n",
    "print(f\"------------------------------ {f} fold {e} epoch------------------------------\")\n",
    "\n",
    "model.train()\n",
    "epoch_perform, batch_perform = np.zeros(2), np.zeros(2)\n",
    "print()\t\n",
    "progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), leave=True, position=0,)\n",
    "for j, v in progress_bar:\n",
    "  input_ids, attention_mask, labels = v['input_ids'].to(device), v['attention_mask'].to(device), v['labels'].to(device)\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  outputs = model(input_ids, attention_mask) ## label을 안 넣어서 logits값만 출력\n",
    "  output = outputs.logits # The outputs object is a SequenceClassifierOutput\n",
    "  loss = criterion(output, labels)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  scheduler.step()\n",
    "  for learning_rate in scheduler.get_lr():\n",
    "    wandb.log({\"learning_rate\": learning_rate})\n",
    "\n",
    "  predict = output.argmax(dim=-1)\n",
    "  predict = predict.detach().cpu().numpy()\n",
    "  labels = labels.detach().cpu().numpy()\n",
    "  acc = accuracy_score(labels, predict)\n",
    "\n",
    "  batch_perform += np.array([loss.item(), acc])\n",
    "  epoch_perform += np.array([loss.item(), acc])\n",
    "\n",
    "  if (j + 1) % 50 == 0:\n",
    "    print(\n",
    "        f\"Epoch {e} #{j + 1} -- loss: {batch_perform[0] / 50}, acc: {batch_perform[1] / 50}\"\n",
    "    )\n",
    "    batch_perform = np.zeros(2)\n",
    "print()\n",
    "print(\n",
    "    f\"Epoch {e} loss: {epoch_perform[0] / total_batch_}, acc: {epoch_perform[1] / total_batch_}\"\n",
    "    )\n",
    "wandb.log({\n",
    "    \"epoch\": e,\n",
    "    \"Train epoch Loss\": epoch_perform[0] / total_batch_,\n",
    "    \"Train epoch Acc\": epoch_perform[1] / total_batch_}\n",
    "    )\n",
    "torch.save(model.state_dict(), f\"./models/{args.model_name}/{f}-fold/train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59338c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 4\n",
    "e = 1\n",
    "best_val_loss, best_val_acc, = np.inf, 0\n",
    "# Validation\n",
    "load_path = f'./models/{args.model_name}/{f}-fold/train.pt'\n",
    "model.load_state_dict(torch.load(load_path,map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "valid_perform = np.zeros(2)\n",
    "\n",
    "all_valid_predict_lst = []\n",
    "all_valid_labels_lst = []\n",
    "\n",
    "# wandb에 잘못 분류하는 코드 Pair 기록\n",
    "wrong_sample_dict = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for v in validloader:\n",
    "      input_ids, attention_mask, valid_labels = v[\"input_ids\"].to(device), v[\"attention_mask\"].to(device), v[\"labels\"].to(device)\n",
    "      \n",
    "      valid_outputs = model(input_ids, attention_mask)\n",
    "      valid_output = valid_outputs.logits\n",
    "      valid_loss = criterion(valid_output, valid_labels)\n",
    "      \n",
    "      valid_predict = valid_output.argmax(dim=-1)\n",
    "      valid_predict = valid_predict.detach().cpu().numpy()\n",
    "      valid_labels = valid_labels.detach().cpu().numpy()\n",
    "\n",
    "      \n",
    "      # wandb에 잘못 분류하는 코드 Pair 기록\n",
    "      if args.logging_wrong_samples:\n",
    "        wrong_sample_index = np.where(valid_labels!=valid_predict)[0]\n",
    "        if len(wrong_sample_index)>0:\n",
    "          wrong_sample_text, wrong_sample_label, wrong_sample_pred, entailment_prob, contradiction_prob = wrong_batch_for_wandb(tokenizer, wrong_sample_index, input_ids, valid_labels, valid_predict, valid_output)\n",
    "\n",
    "          wrong_sample_dict['입력 코드 Pair'] += wrong_sample_text\n",
    "          wrong_sample_dict['실제값'] += wrong_sample_label\n",
    "          wrong_sample_dict['예측값'] += wrong_sample_pred\n",
    "          wrong_sample_dict['diff_logit'] += entailment_prob\n",
    "          wrong_sample_dict['same_logit'] += contradiction_prob\n",
    "\n",
    "\n",
    "      valid_acc = accuracy_score(valid_labels, valid_predict)\n",
    "      valid_perform += np.array([valid_loss.item(), valid_acc])\n",
    "\n",
    "      all_valid_predict_lst += list(valid_predict)\n",
    "      all_valid_labels_lst += list(valid_labels)\n",
    "\n",
    "# Model 저장\n",
    "\n",
    "val_total_loss = valid_perform[0] / valid_batch_\n",
    "val_total_acc = valid_perform[1] / valid_batch_\n",
    "best_val_loss = min(best_val_loss, val_total_loss)\n",
    "\n",
    "\n",
    "if val_total_acc > best_val_acc:\n",
    "    print(f\"New best model for val accuracy : {val_total_acc}! saving the best model..\")\n",
    "    torch.save(model.state_dict(), f\"./models/{args.model_name}/{f}-fold/best.pt\")\n",
    "\n",
    "    best_val_acc = val_total_acc\n",
    "\n",
    "    # wandb에 Confusion Matrix 생성\n",
    "    class_names = ['diff','same'] # (0,1)\n",
    "    wandb.log({f\"{e}_epoch_conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                      y_true=all_valid_labels_lst, preds=all_valid_predict_lst,\n",
    "                                                                      class_names=class_names)})\n",
    "      \n",
    "    if args.logging_wrong_samples and val_total_acc > 0.91:\n",
    "\n",
    "      wrong_sample_df = pd.DataFrame(wrong_sample_dict)\n",
    "      wrong_sample_df.to_csv(f\"./models/{args.model_name}/{f}-fold/wrong_df.csv\",index=False)\n",
    "      print('='*15,f'Fold{f} Wrong DataFrame Saved','='*15)\n",
    "      text_table = wandb.Table(data = wrong_sample_df)\n",
    "      run.log({f\"{f}_fold_wrong_samples\" : text_table})\n",
    "    \n",
    "print()\n",
    "print(\n",
    "    f\">>>> Validation loss: {val_total_loss}, Acc: {val_total_acc}\"\n",
    "    )\n",
    "print()\n",
    "wandb.log({\n",
    "    \"epoch\": e,\n",
    "    \"Last_Valid Loss\": val_total_loss,\n",
    "    \"Last_Valid Acc\": val_total_acc,\n",
    "    })\n",
    "best_val_acc_list.append(best_val_acc)\n",
    "print('='*50)\n",
    "print(f\"{f}fold best_val_acc_list : {best_val_acc_list}\")\n",
    "print('='*15, f'{f}fold Final Score(ACC) : {np.mean(best_val_acc_list)}', '='*15)\n",
    "wandb.log({\n",
    "f\"Total Mean ACC ({f}fold)\": np.mean(best_val_acc_list)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f94c90",
   "metadata": {},
   "source": [
    "## fold 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab 런타임문제로 cell 단위로 나누어 진행했습니다.\n",
    "f = 5\n",
    "\n",
    "print(f\"---------------------------------- {f} fold----------------------------------\")\n",
    "\n",
    "run = wandb.init(project=args.project_name)\n",
    "wandb.run.name = f'{args.model_name}/{f}-fold'\n",
    "wandb.config.update(args)\n",
    "os.makedirs(f'./models/{args.model_name}/{f}-fold', exist_ok=True)\n",
    "\n",
    "total_size = len(dataset)\n",
    "total_ids = list(range(total_size))\n",
    "del_ids = list(range((f-1)*gap, f*gap))\n",
    "training_ids = set(total_ids) - set(del_ids)\n",
    "\n",
    "training_dset = dataset.select(list(training_ids))\n",
    "eval_dset = dataset.select(del_ids)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainloader = DataLoader(training_dset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=True,\n",
    "                          collate_fn = collator\n",
    "                          )\n",
    "\n",
    "validloader = DataLoader(eval_dset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=False,\n",
    "                          collate_fn = collator\n",
    "                          )\n",
    "\n",
    "total_batch_ = len(trainloader)\n",
    "valid_batch_ = len(validloader)\n",
    "\n",
    "optimizer = get_optimizer(model, args)\n",
    "scheduler = get_scheduler(optimizer, args, total_batch_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b2855",
   "metadata": {},
   "source": [
    "## epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed367c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 5\n",
    "e = 1\n",
    "\n",
    "print(f\"------------------------------ {f} fold {e} epoch------------------------------\")\n",
    "\n",
    "model.train()\n",
    "epoch_perform, batch_perform = np.zeros(2), np.zeros(2)\n",
    "print()\t\n",
    "progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), leave=True, position=0,)\n",
    "for j, v in progress_bar:\n",
    "  input_ids, attention_mask, labels = v['input_ids'].to(device), v['attention_mask'].to(device), v['labels'].to(device)\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  outputs = model(input_ids, attention_mask) ## label을 안 넣어서 logits값만 출력\n",
    "  output = outputs.logits # The outputs object is a SequenceClassifierOutput\n",
    "  loss = criterion(output, labels)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  scheduler.step()\n",
    "  for learning_rate in scheduler.get_lr():\n",
    "    wandb.log({\"learning_rate\": learning_rate})\n",
    "\n",
    "  predict = output.argmax(dim=-1)\n",
    "  predict = predict.detach().cpu().numpy()\n",
    "  labels = labels.detach().cpu().numpy()\n",
    "  acc = accuracy_score(labels, predict)\n",
    "\n",
    "  batch_perform += np.array([loss.item(), acc])\n",
    "  epoch_perform += np.array([loss.item(), acc])\n",
    "\n",
    "  if (j + 1) % 50 == 0:\n",
    "    print(\n",
    "        f\"Epoch {e} #{j + 1} -- loss: {batch_perform[0] / 50}, acc: {batch_perform[1] / 50}\"\n",
    "    )\n",
    "    batch_perform = np.zeros(2)\n",
    "print()\n",
    "print(\n",
    "    f\"Epoch {e} loss: {epoch_perform[0] / total_batch_}, acc: {epoch_perform[1] / total_batch_}\"\n",
    "    )\n",
    "wandb.log({\n",
    "    \"epoch\": e,\n",
    "    \"Train epoch Loss\": epoch_perform[0] / total_batch_,\n",
    "    \"Train epoch Acc\": epoch_perform[1] / total_batch_}\n",
    "    )\n",
    "torch.save(model.state_dict(), f\"./models/{args.model_name}/{f}-fold/train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3481c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 5\n",
    "e = 1\n",
    "best_val_loss, best_val_acc, = np.inf, 0\n",
    "# Validation\n",
    "load_path = f'./models/{args.model_name}/{f}-fold/train.pt'\n",
    "model.load_state_dict(torch.load(load_path,map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "valid_perform = np.zeros(2)\n",
    "\n",
    "all_valid_predict_lst = []\n",
    "all_valid_labels_lst = []\n",
    "\n",
    "# wandb에 잘못 분류하는 코드 Pair 기록\n",
    "wrong_sample_dict = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for v in validloader:\n",
    "      input_ids, attention_mask, valid_labels = v[\"input_ids\"].to(device), v[\"attention_mask\"].to(device), v[\"labels\"].to(device)\n",
    "      \n",
    "      valid_outputs = model(input_ids, attention_mask)\n",
    "      valid_output = valid_outputs.logits\n",
    "      valid_loss = criterion(valid_output, valid_labels)\n",
    "      \n",
    "      valid_predict = valid_output.argmax(dim=-1)\n",
    "      valid_predict = valid_predict.detach().cpu().numpy()\n",
    "      valid_labels = valid_labels.detach().cpu().numpy()\n",
    "\n",
    "      \n",
    "      # wandb에 잘못 분류하는 코드 Pair 기록\n",
    "      if args.logging_wrong_samples:\n",
    "        wrong_sample_index = np.where(valid_labels!=valid_predict)[0]\n",
    "        if len(wrong_sample_index)>0:\n",
    "          wrong_sample_text, wrong_sample_label, wrong_sample_pred, entailment_prob, contradiction_prob = wrong_batch_for_wandb(tokenizer, wrong_sample_index, input_ids, valid_labels, valid_predict, valid_output)\n",
    "\n",
    "          wrong_sample_dict['입력 코드 Pair'] += wrong_sample_text\n",
    "          wrong_sample_dict['실제값'] += wrong_sample_label\n",
    "          wrong_sample_dict['예측값'] += wrong_sample_pred\n",
    "          wrong_sample_dict['diff_logit'] += entailment_prob\n",
    "          wrong_sample_dict['same_logit'] += contradiction_prob\n",
    "\n",
    "\n",
    "      valid_acc = accuracy_score(valid_labels, valid_predict)\n",
    "      valid_perform += np.array([valid_loss.item(), valid_acc])\n",
    "\n",
    "      all_valid_predict_lst += list(valid_predict)\n",
    "      all_valid_labels_lst += list(valid_labels)\n",
    "\n",
    "# Model 저장\n",
    "\n",
    "val_total_loss = valid_perform[0] / valid_batch_\n",
    "val_total_acc = valid_perform[1] / valid_batch_\n",
    "best_val_loss = min(best_val_loss, val_total_loss)\n",
    "\n",
    "\n",
    "if val_total_acc > best_val_acc:\n",
    "    print(f\"New best model for val accuracy : {val_total_acc}! saving the best model..\")\n",
    "    torch.save(model.state_dict(), f\"./models/{args.model_name}/{f}-fold/best.pt\")\n",
    "\n",
    "    best_val_acc = val_total_acc\n",
    "\n",
    "    # wandb에 Confusion Matrix 생성\n",
    "    class_names = ['diff','same'] # (0,1)\n",
    "    wandb.log({f\"{e}_epoch_conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                      y_true=all_valid_labels_lst, preds=all_valid_predict_lst,\n",
    "                                                                      class_names=class_names)})\n",
    "      \n",
    "    if args.logging_wrong_samples and val_total_acc > 0.91:\n",
    "\n",
    "      wrong_sample_df = pd.DataFrame(wrong_sample_dict)\n",
    "      wrong_sample_df.to_csv(f\"./models/{args.model_name}/{f}-fold/wrong_df.csv\",index=False)\n",
    "      print('='*15,f'Fold{f} Wrong DataFrame Saved','='*15)\n",
    "      text_table = wandb.Table(data = wrong_sample_df)\n",
    "      run.log({f\"{f}_fold_wrong_samples\" : text_table})\n",
    "    \n",
    "print()\n",
    "print(\n",
    "    f\">>>> Validation loss: {val_total_loss}, Acc: {val_total_acc}\"\n",
    "    )\n",
    "print()\n",
    "wandb.log({\n",
    "    \"epoch\": e,\n",
    "    \"Last_Valid Loss\": val_total_loss,\n",
    "    \"Last_Valid Acc\": val_total_acc,\n",
    "    })\n",
    "best_val_acc_list.append(best_val_acc)\n",
    "print('='*50)\n",
    "print(f\"{f}fold best_val_acc_list : {best_val_acc_list}\")\n",
    "print('='*15, f'{f}fold Final Score(ACC) : {np.mean(best_val_acc_list)}', '='*15)\n",
    "wandb.log({\n",
    "f\"Total Mean ACC ({f}fold)\": np.mean(best_val_acc_list)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492361f5",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684728d",
   "metadata": {},
   "source": [
    "## 테스트 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd4b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_script(code):\n",
    "    \n",
    "    codea = code['code1']\n",
    "    codeb = code['code2']\n",
    "\n",
    "    new_codea = deque()\n",
    "    for line in codea.split('\\n'):\n",
    "        if line.lstrip().startswith('#'): # 주석으로 시작되는 행 skip\n",
    "            continue\n",
    "        line = line.rstrip()\n",
    "        if '#' in line:\n",
    "            line = line[:line.index('#')] # 주석 전까지 코드만 저장\n",
    "        line = line.replace('\\n','')      # 개행 문자를 모두 삭제함\n",
    "        line = line.replace('    ','\\t')  # 공백 4칸을 tab으로 변환\n",
    "\n",
    "        if line == '': # 전처리 후 빈 라인은 skip\n",
    "            continue\n",
    "\n",
    "        new_codea.append(line)\n",
    "\n",
    "    new_codea = '\\n'.join(new_codea)\n",
    "    new_codea = re.sub('(\"\"\"[\\w\\W]*?\"\"\")', '<str>', new_codea)\n",
    "    new_codea = re.sub(\"('''[\\w\\W]*?''')\", '<str>', new_codea)\n",
    "    new_codea = re.sub('/^(http?|https?):\\/\\/([a-z0-9-]+\\.)+[a-z0-9]{2,4}.*$/', '', new_codea)\n",
    "    code['code1'] = new_codea\n",
    "\n",
    "    new_codeb = deque()   \n",
    "    for line in codeb.split('\\n'):\n",
    "        if line.lstrip().startswith('#'): # 주석으로 시작되는 행 skip\n",
    "            continue\n",
    "        line = line.rstrip()\n",
    "        if '#' in line:\n",
    "            line = line[:line.index('#')] # 주석 전까지 코드만 저장\n",
    "        line = line.replace('\\n','')      # 개행 문자를 모두 삭제함\n",
    "        line = line.replace('    ','\\t')  # 공백 4칸을 tab으로 변환\n",
    "\n",
    "        if line == '': # 전처리 후 빈 라인은 skip\n",
    "            continue\n",
    "\n",
    "        new_codeb.append(line)\n",
    "\n",
    "    new_codeb = '\\n'.join(new_codeb)\n",
    "    new_codeb = re.sub('(\"\"\"[\\w\\W]*?\"\"\")', '<str>', new_codeb)\n",
    "    new_codeb = re.sub(\"('''[\\w\\W]*?''')\", '<str>', new_codeb)\n",
    "    new_codeb = re.sub('/^(http?|https?):\\/\\/([a-z0-9-]+\\.)+[a-z0-9]{2,4}.*$/', '', new_codeb)\n",
    "    \n",
    "    code['code2'] = new_codeb\n",
    "    return code\n",
    "\n",
    "\n",
    "def example_fn(examples):\n",
    "    outputs = tokenizer(examples['code1'], examples['code2'], padding='max_length', max_length=512, truncation=True)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65610a",
   "metadata": {},
   "source": [
    "## inference 및 voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdataset = load_dataset(\"csv\", data_files='/content/drive/MyDrive/test.csv')['train']\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "tokenizer.truncation_side = 'left'\n",
    "\n",
    "\n",
    "preprocessed = testdataset.map(preprocess_script)\n",
    "test_dataset = preprocessed.map(example_fn, remove_columns=['code1', 'code2'])\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "testloader = DataLoader(test_dataset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=False,\n",
    "                         collate_fn = collator\n",
    "                          )\n",
    "\n",
    "all_fold_logits = np.zeros((179700, 2))  # rows of df, target labels\n",
    "for idx in tqdm(range(1, args.n_splits+1)):\n",
    "  if idx == 6: # 6번째 모델은 별도로 저장된 모델 불러옴\n",
    "    checkpoint_path = f'./models/{args.model_name}/{idx}-fold/checkpoint-66000'\n",
    "    config =  AutoConfig.from_pretrained(checkpoint_path)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(checkpoint_path, config=config)\n",
    "    model = model.to(device)\n",
    "  else:\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "    load_path = f'./models/{args.model_name}/{idx}-fold/best.pt'\n",
    "    model.load_state_dict(torch.load(load_path,map_location=torch.device('cpu')))\n",
    "    model.to(device)\n",
    "  \n",
    "  model.eval()\n",
    "  progress_bar = tqdm(enumerate(testloader), total=len(testloader), leave=True, position=0,)\n",
    "  for i, data in progress_bar:\n",
    "    with torch.no_grad():\n",
    "      logits = model(\n",
    "                  data['input_ids'].to(device),\n",
    "                  data['attention_mask'].to(device),\n",
    "                  )\n",
    "      logits=logits.logits\n",
    "    if i==0:\n",
    "      one_fold_logits = logits\n",
    "    else:\n",
    "      one_fold_logits = torch.cat([one_fold_logits,logits],dim=0)\n",
    "\n",
    "  # torch tensor를 저장하기 위한 numpy 변환\n",
    "  one_fold_logits = one_fold_logits.squeeze(0).detach().cpu().numpy()\n",
    "  # numpy array 저장\n",
    "  np.save(f'./models/{args.model_name}/{idx}-fold/numpy_logits', one_fold_logits)\n",
    "  \n",
    "  all_fold_logits += one_fold_logits\n",
    "  if idx == 1:\n",
    "    all_fold_predictions = np.argmax(one_fold_logits, axis=1)\n",
    "  else:\n",
    "    one_fold_predictions = np.argmax(one_fold_logits, axis=1)\n",
    "    all_fold_predictions = np.vstack([all_fold_predictions, one_fold_predictions])\n",
    "\n",
    "soft_output = list(np.argmax(all_fold_logits, axis=1))\n",
    "hard_output = ([max(list(Counter(lst).items()), key=lambda x:x[1])[0] for lst in all_fold_predictions.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e443c4d",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = \"/content/drive/MyDrive/sample_submission.csv\"\n",
    "\n",
    "submissionsoft = pd.read_csv(submission_path)\n",
    "submissionhard = pd.read_csv(submission_path)\n",
    "\n",
    "submissionsoft['similar']=soft_output\n",
    "submissionhard['similar']=hard_output\n",
    "\n",
    "submissionsoft.to_csv('/content/drive/MyDrive/submissionsoft.csv', index=False)\n",
    "submissionhard.to_csv('/content/drive/MyDrive/submissionhard.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
